{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7950902",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97be64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c7f3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.memory import MemoryContent, MemoryMimeType\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d1763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\n",
    "    \"https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md\",\n",
    "    \"https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html\",\n",
    "    \"https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html\",\n",
    "    \"https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2139e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_user_memory = ChromaDBVectorMemory(\n",
    "    config=PersistentChromaDBVectorMemoryConfig(\n",
    "        collection_name=\"user_memory\",\n",
    "        persistence_path=os.getenv(\"CHROMA_PERSISTENCE_PATH\", \"user_memory\"),\n",
    "        k=3,\n",
    "        score_threshold=0.5,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81fc91cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: module 'chromadb' has no attribute 'get_settings'\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: module 'chromadb' has no attribute 'get_settings'\n",
      "Failed to send telemetry event CollectionGetEvent: module 'chromadb' has no attribute 'get_settings'\n"
     ]
    }
   ],
   "source": [
    "await chroma_user_memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b8adeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:10<00:00, 8.17MiB/s]\n",
      "Failed to send telemetry event CollectionAddEvent: module 'chromadb' has no attribute 'get_settings'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 69 chunks from 4 documents.\n"
     ]
    }
   ],
   "source": [
    "from index_utils import SimpleDocumentIndexer\n",
    "\n",
    "\n",
    "async def index_autogen_docs() -> None:\n",
    "    indexer = SimpleDocumentIndexer(\n",
    "        memory=chroma_user_memory,\n",
    "    )\n",
    "\n",
    "    chunks: int = await indexer.index_documents(sources)\n",
    "\n",
    "    print(f\"Indexed {chunks} chunks from {len(sources)} documents.\")\n",
    "\n",
    "await index_autogen_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb77558",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_assistant = AssistantAgent(\n",
    "    name=\"RAGAssistant\",\n",
    "    description=\"An assistant that can answer questions using the RAG approach.\",\n",
    "    model_client=AzureOpenAIChatCompletionClient(\n",
    "        azure_deployment=os.getenv(\"deployment_name\"),\n",
    "        model=os.getenv(\"model_name\"),\n",
    "        api_version=os.getenv(\"api_version\"),\n",
    "        azure_endpoint=os.getenv(\"azure_endpoint\"),\n",
    "        api_key=os.getenv(\"api_key\")\n",
    "    ),\n",
    "    memory = [chroma_user_memory],\n",
    "    system_message=\"You are a helpful assistant that can answer questions using the RAG approach. \"\n",
    "                   \"You have access to a memory that contains relevant information.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0918b8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "What is Agent Chat in AutoGen?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: module 'chromadb' has no attribute 'get_settings'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- MemoryQueryEvent (RAGAssistant) ----------\n",
      "[MemoryContent(content='arch Literature Review API Reference PyPi Source AgentChat Agents Agents # AutoGen AgentChat provides a set of preset Agents, each with variations in how an agent might respond to messages. All agents share the following attributes and methods: name : The unique name of the agent. description : The description of the agent in text. run : The method that runs the agent given a task as a string or a list of messages, and returns a TaskResult . Agents are expected to be stateful and this method is expected to be called with new messages, not complete history . run_stream : Same as run() but returns an iterator of messages that subclass BaseAgentEvent or BaseChatMessage followed by a TaskResult as the last item. See autogen_agentchat.messages for more information on AgentChat message types. Assistant Agent # AssistantAgent is a built-in agent that uses a language model and has the ability to use tools. Warning AssistantAgent is a “kitchen sink” agent for prototyping and educational purpose – it is very general. Make sure you read the documentation and implementation to understand the design choices. Once you fully understand the design, you may want to implement your own agent. See Custom Agent . from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.messages import StructuredMessage from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient # Define a tool that searches the web for information. # For simplicity,', mime_type='MemoryMimeType.TEXT', metadata={'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html', 'mime_type': 'MemoryMimeType.TEXT', 'chunk_index': 1, 'score': 0.6224494874477386, 'id': '115a154b-a90a-426d-aedb-4b273578b925'}), MemoryContent(content='anguage support for .NET and Python. - [AgentChat API](./python/packages/autogen-agentchat/) implements a simpler but opinionated API for rapid prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports common multi-agent patterns such as two-agent chat or group chats. - [Extensions API](./python/packages/autogen-ext/) enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution. The ecosystem also supports two essential _developer tools_: - [AutoGen Studio](./python/packages/autogen-studio/) provides a no-code GUI for building multi-agent applications. - [AutoGen Bench](./python/packages/agbench/) provides a benchmarking suite for evaluating agent performance. You can use the AutoGen framework and developer tools to create applications for your domain. For example, [Magentic-One](./python/packages/magentic-one-cli/) is a state-of-the-art multi-agent team built using AgentChat API and Extensions API that can handle a variety of tasks that require web browsing, code execution, and file handling. With AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a [Discord server](https://aka.ms/autogen-discord) for real-time chat, GitHub Discussions for Q&A, and a blog for tutorials and', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'chunk_index': 3, 'source': 'https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md', 'score': 0.6129800379276276, 'id': '96b352ee-2a49-4a5d-909e-8bef691c0f5b'}), MemoryContent(content='multi-agent applications.&#39;, name=&#39;web_search&#39;, call_id=&#39;call_703i17OLXfztkuioUbkESnea&#39;, is_error=False)], type=&#39;ToolCallExecutionEvent&#39;), ToolCallSummaryMessage(source=&#39;assistant&#39;, models_usage=None, metadata={}, content=&#39;AutoGen is a programming framework for building multi-agent applications.&#39;, type=&#39;ToolCallSummaryMessage&#39;)] The call to the run() method returns a TaskResult with the list of messages in the messages attribute, which stores the agent’s “thought process” as well as the final response. Note It is important to note that run() will update the internal state of the agent – it will add the messages to the agent’s message history. You can also call run() without a task to get the agent to generate responses given its current state. Note Unlike in v0.2 AgentChat, the tools are executed by the same agent directly within the same call to run() . By default, the agent will return the result of the tool call as the final response. Multi-Modal Input # The AssistantAgent can handle multi-modal input by providing the input as a MultiModalMessage . from io import BytesIO import PIL import requests from autogen_agentchat.messages import MultiModalMessage from autogen_core import Image # Create a multi-modal message with random image and text. pil_image = PIL . Image . open ( BytesIO ( requests . get ( &quot;https://picsum.photos/300/200&quot; ) . content )) img = Image ( pil_image ) multi_modal_message = MultiModalMessage', mime_type='MemoryMimeType.TEXT', metadata={'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html', 'mime_type': 'MemoryMimeType.TEXT', 'chunk_index': 3, 'score': 0.5910254120826721, 'id': '14528cd1-4570-42b8-9cc1-5fdeb2b11208'})]\n",
      "---------- TextMessage (RAGAssistant) ----------\n",
      "Agent Chat in AutoGen is a framework designed to facilitate the creation and prototyping of multi-agent applications. It provides an opinionated API for implementing, managing, and interacting with agents in conversations or tasks. Here are some of its key features:\n",
      "\n",
      "1. **Agent Attributes and Methods**:\n",
      "    - Each agent has a unique `name` and a textual `description`.\n",
      "    - The `run` method enables an agent to complete tasks based on an input string or a list of messages, returning a `TaskResult`. Each agent is stateful, meaning its internal message history is updated with new messages.\n",
      "    - The `run_stream` method provides a real-time, stream-based interaction with an agent, returning events or messages as an iterator.\n",
      "\n",
      "2. **Assistant Agent**:\n",
      "   - The framework includes a built-in general-purpose agent called the `AssistantAgent`. This agent uses a language model and can leverage tools to complete tasks.\n",
      "   - It’s designed for prototyping and educational purposes, and users can customize it or create their own agents after understanding its design.\n",
      "\n",
      "3. **Multi-Modal Input**:\n",
      "   - The AssistantAgent supports handling multi-modal inputs, such as text combined with images or other media, using the `MultiModalMessage`.\n",
      "\n",
      "4. **Tool Integration**:\n",
      "   - Agents can directly execute tools and use their outputs within the same method calls, providing seamless integration of external capabilities like code execution or web browsing.\n",
      "\n",
      "Agent Chat is built on AutoGen’s Core API and Extensions API, enabling a variety of multi-agent patterns including two-agent chats or group chats. Developers can expand capabilities or even build ecosystems, such as [Magentic-One](https://python/packages/magentic-one-cli/), which demonstrates advanced functionality for tasks like web browsing and file handling.\n",
      "RAG Assistant has finished running. You can now ask questions using the RAG approach.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/advanced/.venv/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py:981: UserWarning: Resolved model mismatch: gpt-4o-2024-08-06 != gpt-4o-2024-11-20. Model mapping in autogen_ext.models.openai may be incorrect. Set the model to gpt-4o-2024-11-20 to enhance token/cost estimation and suppress this warning.\n",
      "  model_result = await model_client.create(\n"
     ]
    }
   ],
   "source": [
    "stream = rag_assistant.run_stream(\n",
    "    task=\"What is Agent Chat in AutoGen?\"\n",
    ")\n",
    "\n",
    "await Console(stream)\n",
    "\n",
    "await chroma_user_memory.close()\n",
    "\n",
    "print(\"RAG Assistant has finished running. You can now ask questions using the RAG approach.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
